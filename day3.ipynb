{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TurKClYFLjnc"
      },
      "source": [
        "#Day 3 - Data Preparation & Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM4hSusSLoSS"
      },
      "source": [
        "## cleaning data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5yUrZcVK1ek"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/mini-bootcamp-bisaai/dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yLAx6cJLW22"
      },
      "source": [
        "#  Cleaning text\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(tweet):\n",
        "    tweet = tweet.lower() # text lowercase\n",
        "    tweet = re.sub('@[^\\s]+', '', tweet) # remove usernames\n",
        "    tweet = re.sub('\\[.*?\\]', '', tweet) # remove square brackets\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet) # remove URLs\n",
        "    tweet = re.sub('[%s]' % re.escape(string.punctuation), '', tweet) # remove punctuation\n",
        "    tweet = re.sub('\\w*\\d\\w*', '', tweet) \n",
        "    tweet = re.sub('[‘’“”…]', '', tweet)\n",
        "    tweet = re.sub('\\n', '', tweet)\n",
        "    return tweet\n",
        "\n",
        "    \n",
        "tweet = lambda x: clean_text(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ3_XX5iLX6c"
      },
      "source": [
        "df['clean1'] = pd.DataFrame(df.content.apply(tweet))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os3A4OlgLZbF"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "additional  = ['rt','rts','retweet']\n",
        "swords = set().union(stopwords.words('indonesian'), additional)\n",
        "\n",
        "df['clean2'] = (df['clean1'].apply(lambda x: ' '.join([word for word in x.split() if word not in (swords)])))\n",
        "dfdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN0QWRYeLaWR"
      },
      "source": [
        "text = df['clean2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_SnSxd1LbEF"
      },
      "source": [
        "!pip install Sastrawi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhXystOVLcDV"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "list_hasil = text\n",
        "\n",
        "output = [(stemmer.stem(token)) for token in list_hasil]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjzR58B4LdRy"
      },
      "source": [
        "df['clean3'] = output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg4eZQkgLd9S"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq23dmKnLe3B"
      },
      "source": [
        "df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhtuIgapLgGf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5zFoopYLgfr"
      },
      "source": [
        "df['tokens'] = pd.DataFrame(df['clean3'].apply(nltk.word_tokenize))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYk-YgAgLrEK"
      },
      "source": [
        "## translate to english"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuJgwotwLs9y"
      },
      "source": [
        "!pip install google_trans_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voTxHclALuLz"
      },
      "source": [
        "from google_trans_new import google_translator  \n",
        "\n",
        "translator = google_translator()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge2ym-b1Lux3"
      },
      "source": [
        "detect_text = translator.detect('apa kabar?')  \n",
        "detect_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkIaIwJwLvc0"
      },
      "source": [
        "translate_text = translator.translate('apa kabar ?', lang_tgt='en')\n",
        "translate_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjRNc4J4LwAh"
      },
      "source": [
        "def translate_column(text, target_language):\n",
        "    return translator.translate(text, lang_tgt=target_language)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIsMoZqsL9l8"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9hBiwnoL3wG"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/mini-bootcamp/dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5kMS2-xLwnW"
      },
      "source": [
        "df['clean_english'] = df['content'].apply(lambda x: translate_column(x, 'en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBaqwWhTLxXw"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbwFUueMLydF"
      },
      "source": [
        "df.to_csv('/content/drive/MyDrive/mini-bootcamp-bisaai/dataset_clean.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}